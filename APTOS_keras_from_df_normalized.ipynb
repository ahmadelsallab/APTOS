{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"\"\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.autonotebook import tqdm","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nroot = '/kaggle/input/aptos2019-blindness-detection/'\ntrain_img_path = os.path.join(root,'train_images')\ntrain_path = os.path.join(root,'train.csv')\ntest_img_path = os.path.join(root,'test_images')\ntest_path = os.path.join(root,'test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv(train_path)\ntrain_data.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"        id_code  diagnosis\n0  000c1434d8d7          2\n1  001639a390f0          4\n2  0024cdab0c1e          1\n3  002c21358ce6          0\n4  005b95c28852          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000c1434d8d7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001639a390f0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0024cdab0c1e</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>002c21358ce6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>005b95c28852</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.hist()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa5d770f4e0>]],\n      dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = train_data.diagnosis.nunique()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = train_data.diagnosis.unique()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.diagnosis.value_counts()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"0    1805\n2     999\n1     370\n4     295\n3     193\nName: diagnosis, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['id_code'] = train_data['id_code'].apply(lambda x:x+'.png')\ntrain_data['diagnosis'] = train_data['diagnosis'].apply(lambda x:str(x))\ntrain_data.head()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"            id_code diagnosis\n0  000c1434d8d7.png         2\n1  001639a390f0.png         4\n2  0024cdab0c1e.png         1\n3  002c21358ce6.png         0\n4  005b95c28852.png         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000c1434d8d7.png</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001639a390f0.png</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0024cdab0c1e.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>002c21358ce6.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>005b95c28852.png</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dtypes","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"id_code      object\ndiagnosis    object\ndtype: object"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the size of our input data\nsz=224","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import layers\nfrom keras import models\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\n                        input_shape=(sz, sz, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(n_classes, activation='softmax'))","execution_count":12,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":13,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 109, 109, 64)      18496     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 52, 52, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 26, 26, 128)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 24, 24, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 12, 12, 128)       0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 18432)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               9437696   \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 2565      \n=================================================================\nTotal params: 9,681,093\nTrainable params: 9,681,093\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\n\n\n'''\nopt = Adam(lr=1e-4)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n'''\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['acc'])","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import keras\n#from keras.models import Sequential, Model \nfrom keras.preprocessing.image import ImageDataGenerator\n#from keras.applications.vgg16 import VGG16, preprocess_input\n#from keras.layers import Dropout, Flatten,Dense\n\n\nimport numpy as np\nimport os\nfrom matplotlib import image,patches,patheffects\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# our batch size\nbs=32\n\n# No need for preprocess_fn as it will be handled in target_sz of the generator\n\nimport cv2\ndef preprocess_input(img):\n    #return img/255#cv2.resize(img, (sz,sz))\n    return img.astype('float32')/255\n\n# preprocess_input is for VGG16 in our case\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                   rotation_range=20,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.1,\n                                   zoom_range=0.1,\n                                   horizontal_flip=True) \n\nvalid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) \n\n\ntrain_batches = train_datagen.flow_from_dataframe(train_df, # The df\n                                                  train_img_path, # Place on desk\n                                                  x_col='id_code', # The column to get x\n                                                  y_col='diagnosis', # The column to get y\n                                                  #has_ext=True, \n                                                  target_size=(sz, sz), \n                                                  color_mode='rgb', \n                                                  classes=None, \n                                                  class_mode='categorical', \n                                                  batch_size=bs, \n                                                  shuffle=True)\n\n\n\nvalid_batches = valid_datagen.flow_from_dataframe(val_df, \n                                                  train_img_path, \n                                                  x_col='id_code', \n                                                  y_col='diagnosis', \n                                                  #has_ext=True, \n                                                  target_size=(sz, sz), \n                                                  color_mode='rgb', \n                                                  classes=None,#list(train_batches.class_indices),#classes, \n                                                  class_mode='categorical', \n                                                  batch_size=bs, \n                                                  shuffle=False)\n\nNbClasses = len(train_batches.class_indices)","execution_count":16,"outputs":[{"output_type":"stream","text":"Found 2929 validated image filenames belonging to 5 classes.\nFound 733 validated image filenames belonging to 5 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\nfrom keras.callbacks import Callback\nclass Kappa(Callback):\n    \n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, y_val = self.validation_data[0], self.validation_data[1]\n        y_predict = np.asarray(model.predict(X_val))\n\n        y_val = np.argmax(y_val, axis=1)\n        y_predict = np.argmax(y_predict, axis=1)\n\n        self._data.append({\n            'val_kappa': cohen_kappa_score(y_val, y_predict),\n        })\n        return\n\n    def get_data(self):\n        return self._data","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    quadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return 1.0 - numerator / denominator","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val = []\ny_val = []\ns = 0\nvalidation_steps = valid_batches.n // valid_batches.batch_size\nprint(validation_steps)\nfor b in tqdm(valid_batches):\n#for s in validation_steps:\n    #b = next(valid_batches)\n    #print(s)\n    if s > validation_steps: # to prevent looping forever\n        break\n    s+=1\n    X_val.append(b[0])\n    y_val.append(b[1])\nX_val = np.concatenate(X_val)    \ny_val = np.concatenate(y_val) \n   ","execution_count":19,"outputs":[{"output_type":"stream","text":"22\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8cb525901c54943a5464031082f8bff"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_val.shape)\nprint(y_val.shape)","execution_count":20,"outputs":[{"output_type":"stream","text":"(733, 224, 224, 3)\n(733, 5)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import cohen_kappa_score\n\nclass QWKP(Callback):\n    \n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        #X_val, y_val = self.validation_data[0], self.validation_data[1]\n\n        \n        y_predict = np.asarray(model.predict(X_val))\n\n        y_val_ = np.argmax(y_val, axis=1)\n        y_predict = np.argmax(y_predict, axis=1)\n        qwkp =  quadratic_weighted_kappa(y_val_, y_predict)\n        print('Validation qwkp: ', str(qwkp))\n        self._data.append({\n            'val_kappa':qwkp,\n        })\n        return\n\n    def get_data(self):\n        return self._data","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\n\nhistory = model.fit_generator(train_batches,\n                              steps_per_epoch = train_batches.n // train_batches.batch_size,\n                              epochs=epochs,\n                              #validation_data=valid_batches,\n                              #validation_steps = valid_batches.n // valid_batches.batch_size,\n                              callbacks=[QWKP()])","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n 8/91 [=>............................] - ETA: 8:03 - loss: 1.3591 - acc: 0.4609","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a sample image\nb = next(valid_batches)\nimg = b[0][5]\nprint(img.shape)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax(b[1][5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.expand_dims(img, 0)\npreds = model.predict(x)\nidx = np.argmax(preds[0])\nidx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n# This is the index entry in the prediction vector\noutput = model.output[:, idx]\n\n# The is the output feature map of the `block5_conv3` layer,\n# the last convolutional layer in VGG16\nlast_conv_layer =model.layers[6] #model.get_layer('block5_conv3')\n\n# This is the gradient of the \"african elephant\" class with regard to\n# the output feature map of `block5_conv3`\ngrads = K.gradients(output, last_conv_layer.output)[0]\n\n# This is a vector of shape (512,), where each entry\n# is the mean intensity of the gradient over a specific feature map channel\npooled_grads = K.mean(grads, axis=(0, 1, 2))\n\n# This function allows us to access the values of the quantities we just defined:\n# `pooled_grads` and the output feature map of `block5_conv3`,\n# given a sample image\niterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n\n# These are the values of these two quantities, as Numpy arrays,\n# given our sample image of two elephants\npooled_grads_value, conv_layer_output_value = iterate([x])\n\n# We multiply each channel in the feature map array\n# by \"how important this channel is\" with regard to the elephant class\nfor i in range(128):\n    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n# The channel-wise mean of the resulting feature map\n# is our heatmap of class activation\nheatmap = np.mean(conv_layer_output_value, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap = np.maximum(heatmap, 0)\nheatmap /= np.max(heatmap)\nplt.matshow(heatmap)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\n\n\n# We resize the heatmap to have the same size as the original image\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n# We convert the heatmap to RGB\nheatmap = np.uint8(255 * heatmap)\n#eatmap = np.uint8(heatmap)\n\n# We apply the heatmap to the original image\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n# 0.4 here is a heatmap intensity factor\nsuperimposed_img = heatmap * 0.1 + img\n\n# Save the image to disk\n#cv2.imwrite('elephant_cam.jpg', superimposed_img)\nplt.imshow(superimposed_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nimport cv2\ndef grad_CAM(model, layer, img):\n    x = np.expand_dims(img, 0)\n    preds = model.predict(x)\n    idx = np.argmax(preds[0])\n\n    \n    # This is the index entry in the prediction vector\n    output = model.output[:, idx]\n\n    # The is the output feature map of the `block5_conv3` layer,\n    # the last convolutional layer in VGG16\n    last_conv_layer =model.layers[layer] #model.get_layer('block5_conv3')\n\n    # This is the gradient of the \"african elephant\" class with regard to\n    # the output feature map of `block5_conv3`\n    grads = K.gradients(output, last_conv_layer.output)[0]\n\n    # This is a vector of shape (512,), where each entry\n    # is the mean intensity of the gradient over a specific feature map channel\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n\n    # This function allows us to access the values of the quantities we just defined:\n    # `pooled_grads` and the output feature map of `block5_conv3`,\n    # given a sample image\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n\n    # These are the values of these two quantities, as Numpy arrays,\n    # given our sample image of two elephants\n    pooled_grads_value, conv_layer_output_value = iterate([x])\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the elephant class\n    for i in range(128):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    \n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n    #plt.matshow(heatmap)\n    #plt.show()    \n    \n    # We resize the heatmap to have the same size as the original image\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # We convert the heatmap to RGB\n    heatmap = np.uint8(255 * heatmap)\n    #eatmap = np.uint8(heatmap)\n\n    # We apply the heatmap to the original image\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # 0.4 here is a heatmap intensity factor\n    superimposed_img = heatmap * 0.1 + img\n\n    # Save the image to disk\n    #cv2.imwrite('elephant_cam.jpg', superimposed_img)\n    plt.imshow(superimposed_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(os.path.join(root, 'sample_submission.csv'))\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['id_code'] = submission['id_code'].apply(lambda x:x+'.png')\n#submission['diagnosis'] = submission['diagnosis'].apply(lambda x:str(x))\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(test_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['id_code'] = test_df['id_code'].apply(lambda x:x+'.png')\n#test_df['diagnosis'] = test_df['diagnosis'].apply(lambda x:str(x))\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls {test_img_path}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n'''\nsubmission.diagnosis = submission.diagnosis.apply(str)\ntest_batches = test_datagen.flow_from_dataframe(submission, \n                                                  test_img_path, \n                                                  x_col='id_code', \n                                                  y_col='diagnosis', \n                                                  #has_ext=True, \n                                                  target_size=(sz, sz), \n                                                  color_mode='rgb', \n                                                  classes=list(train_batches.class_indices),#classes, \n                                                  class_mode='categorical', #'input',\n                                                  batch_size=bs, \n                                                  shuffle=False)\n'''\n\ntest_batches = test_datagen.flow_from_dataframe(submission,#test_df, \n                                                  test_img_path, \n                                                  x_col='id_code', \n                                                  #y_col='diagnosis', \n                                                  #has_ext=True, \n                                                  target_size=(sz, sz), \n                                                  color_mode='rgb', \n                                                  #classes=list(train_batches.class_indices),#classes, \n                                                  class_mode=None,#'categorical', #'input',\n                                                  batch_size=1, \n                                                  shuffle=False)\n\npreds = []\n'''\ni = 0\nfor batch in test_batches:\n\n    #print(model.predict(batch))\n    preds.append(model.predict(batch))#(np.argmax(model.predict(batch)))\n    i+=1\n    print(i)\n    if i == len(test_df):\n        break\n'''        \n\nfor i in tqdm(range(len(test_df))):\n    batch = next(test_batches)\n    preds.append(np.argmax(model.predict(batch)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nsubmission['diagnosis'] = preds\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}